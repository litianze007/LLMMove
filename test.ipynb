{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      category    place_id         place_name  comment_id  comment_user_id   \n",
      "0          CE7   756993733  투썸플레이스 디지털미디어시티역점    10840268       2350725315  \\\n",
      "1          CE7   756993733  투썸플레이스 디지털미디어시티역점    10517631       1650546034   \n",
      "2          CE7   756993733  투썸플레이스 디지털미디어시티역점    10349082       3371210705   \n",
      "3          CE7   972496863      메가MGC커피 수색자이점     9461669       1231775476   \n",
      "4          CE7   972496863      메가MGC커피 수색자이점     9246847       1030013027   \n",
      "...        ...         ...                ...         ...              ...   \n",
      "16553      FD6    15528437             광화문수제비     7025846        891589374   \n",
      "16554      FD6  1195558165               리우키친    10188764        418219292   \n",
      "16555      FD6  1195558165               리우키친    10149964       2377789139   \n",
      "16556      FD6  1195558165               리우키친     7963312       2234012693   \n",
      "16557      FD6  1594659846       만평우동 상암KGIT점    11038430        418219292   \n",
      "\n",
      "                                         comment_content  comment_point   \n",
      "0                                                    NaN              2  \\\n",
      "1                                              접근이 편한 투썸              5   \n",
      "2      시간 때우려 방문했는데 너무 더워요.... 1층 로비랑 문없이 연결되어있던데 진짜 ...              3   \n",
      "3                                    마감시간 되지도 않앗는데 눈치줘요ㅜ              1   \n",
      "4                    아메리카노 먹을때은 몰랐는데, 정량을 안지키고 적게줘요..라떼류              1   \n",
      "...                                                  ...            ...   \n",
      "16553  요즘 물가에 가성비 최고👍 바지락이 있어서 국물이 시원함, 대신 만두가 시판인 것 ...              4   \n",
      "16554                              맛과 가성비를 다 잡은 상암동 최고맛집              5   \n",
      "16555  가성비 개 미친 샐러드가게 상암에 오래오래 있어주세요 제발제발제발. 가격 개혜잔데 ...              5   \n",
      "16556                   선결제하고 찾아가는 맛집\\n샐러드는 매우 훌륭함 :-)\\n              5   \n",
      "16557  우동세트 8,700₩\\n전체적으로 양이 많지 않습니다. 맛도 그냥 그렇고요. 문제는...              3   \n",
      "\n",
      "      comment_date        row_id  \n",
      "0      2024.08.22.   756993733_1  \n",
      "1      2024.07.09.   756993733_2  \n",
      "2      2024.06.14.   756993733_3  \n",
      "3      2024.02.02.   972496863_1  \n",
      "4      2024.01.01.   972496863_2  \n",
      "...            ...           ...  \n",
      "16553  2023.02.17.    15528437_3  \n",
      "16554  2024.08.06.  1195558165_1  \n",
      "16555  2024.05.18.  1195558165_2  \n",
      "16556  2023.06.28.  1195558165_3  \n",
      "16557  2024.09.19.  1594659846_1  \n",
      "\n",
      "[16558 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 假设数据存储在 CSV 文件中\n",
    "file_path = \"./datasets/mapogu-4k-ce7-fd6.csv\"  # 替换成你的文件路径\n",
    "\n",
    "# 读取 CSV 文件\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 初始化一个字典来计数每个 place_id 的出现次数\n",
    "place_id_counter = {}\n",
    "\n",
    "# 定义一个函数为每行生成 row_id\n",
    "def generate_row_id(row):\n",
    "    place_id = row['place_id']\n",
    "    if place_id not in place_id_counter:\n",
    "        place_id_counter[place_id] = 1\n",
    "    else:\n",
    "        place_id_counter[place_id] += 1\n",
    "    return f\"{place_id}_{place_id_counter[place_id]}\"\n",
    "\n",
    "# 创建新的 row_id 列\n",
    "df['row_id'] = df.apply(generate_row_id, axis=1)\n",
    "\n",
    "\n",
    "# 打印生成的 DataFrame\n",
    "print(df)\n",
    "\n",
    "# 保存回CSV（可选）\n",
    "df.to_csv(\"./datasets/mapogu-4k-ce7-fd6_row_id.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件中的行数为: 16558\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 设置文件路径\n",
    "file_path = './datasets/mapogu-4k-ce7-fd6_row_id.csv'\n",
    "\n",
    "# 读取 CSV 文件\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 获取行数\n",
    "row_count = df.shape[0]\n",
    "\n",
    "\n",
    "# 输出行数\n",
    "print(f\"文件中的行数为: {row_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件修复完成，结果已写入 ./datasets/fixed_row_id_score.json\n"
     ]
    }
   ],
   "source": [
    "# 读取原始文件，并在开始和结尾添加方括号\n",
    "input_file_path = './datasets/mapogu-4k-ce7-fd6_row_id_score.json'\n",
    "output_file_path = './datasets/fixed_row_id_score.json'\n",
    "\n",
    "try:\n",
    "    # 读取原始文件并逐行写入，确保加入数组结构\n",
    "    with open(input_file_path, 'r') as infile, open(output_file_path, 'w') as outfile:\n",
    "        outfile.write('[')  # 在文件开始添加左方括号\n",
    "        \n",
    "        first_line = True  # 标记是否是第一行\n",
    "        for line in infile:\n",
    "            if not first_line:\n",
    "                outfile.write(',\\n')  # 在每行之间添加逗号分隔\n",
    "            outfile.write(line.strip())\n",
    "            first_line = False\n",
    "        \n",
    "        outfile.write(']')  # 在文件结尾添加右方括号\n",
    "    \n",
    "    print(f\"文件修复完成，结果已写入 {output_file_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"文件处理时发生错误: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已删除 sentiment_score 不在 [-1, 1] 范围内的记录，剩余 13223 条。\n",
      "不符合条件的记录已保存，数量为：613\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "try:\n",
    "    # 读取 JSON 文件\n",
    "    with open('./datasets/fixed_row_id_score.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # 筛选出符合条件和不符合条件的记录\n",
    "    filtered_data = [item for item in data if -1 <= item['sentiment_score'] <= 1]\n",
    "    removed_data = [item for item in data if not (-1 <= item['sentiment_score'] <= 1)]\n",
    "    \n",
    "    # 将筛选后的数据写回文件（或保存为新文件）\n",
    "    with open('./datasets/filtered_row_id_score.json', 'w') as file:\n",
    "        json.dump(filtered_data, file, indent=4)\n",
    "    \n",
    "    # 如果想保留被删除的记录\n",
    "    if removed_data:\n",
    "        with open('./datasets/removed_row_id_score.json', 'w') as file:\n",
    "            json.dump(removed_data, file, indent=4)\n",
    "    \n",
    "    print(f\"已删除 sentiment_score 不在 [-1, 1] 范围内的记录，剩余 {len(filtered_data)} 条。\")\n",
    "    if removed_data:\n",
    "        print(f\"不符合条件的记录已保存，数量为：{len(removed_data)}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"文件未找到，请检查路径是否正确。\")\n",
    "except json.JSONDecodeError:\n",
    "    print(\"JSON 文件格式错误，请检查文件内容。\")\n",
    "except Exception as e:\n",
    "    print(f\"发生错误: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理完成，更新后的CSV文件已保存到 ./datasets/updated_mapogu-4k-ce7-fd6_row_id.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "# 文件路径\n",
    "json_file_path = './datasets/filtered_row_id_score.json'  # 你的JSON文件路径\n",
    "csv_file_path = './datasets/mapogu-4k-ce7-fd6_row_id.csv'  # 你的CSV文件路径\n",
    "output_csv_file_path = './datasets/updated_mapogu-4k-ce7-fd6_row_id.csv'  # 输出文件路径\n",
    "\n",
    "# 读取JSON文件\n",
    "with open(json_file_path, 'r') as json_file:\n",
    "    json_data = json.load(json_file)\n",
    "\n",
    "# 创建一个字典来映射row_id到sentiment_score\n",
    "row_id_to_sentiment = {item['row_id']: item['sentiment_score'] for item in json_data}\n",
    "\n",
    "# 读取CSV文件并逐行处理\n",
    "with open(csv_file_path, 'r', encoding='utf-8') as csv_file, open(output_csv_file_path, 'w', newline='', encoding='utf-8') as output_csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    csv_writer = csv.writer(output_csv_file)\n",
    "    \n",
    "    # 获取表头\n",
    "    header = next(csv_reader)\n",
    "    # 在表头中增加一个sentiment_score列\n",
    "    header.append('sentiment_score')\n",
    "    csv_writer.writerow(header)\n",
    "    \n",
    "    # 处理每一行数据\n",
    "    for row in csv_reader:\n",
    "        row_id = row[-1]  # 假设 row_id 在CSV文件的最后一列\n",
    "        sentiment_score = row_id_to_sentiment.get(row_id, 'null')  # 如果没有找到匹配的row_id，sentiment_score为null\n",
    "        row.append(sentiment_score)\n",
    "        csv_writer.writerow(row)\n",
    "\n",
    "print(f\"处理完成，更新后的CSV文件已保存到 {output_csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理完成，结果已保存到 ./datasets/final_mapogu-4k-ce7-fd6_row_id_scaled.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# 文件路径\n",
    "csv_file_path = './datasets/updated_mapogu-4k-ce7-fd6_row_id.csv'  # 你的现有CSV文件路径\n",
    "output_csv_file_path = './datasets/final_mapogu-4k-ce7-fd6_row_id_scaled.csv'  # 最终输出的文件路径\n",
    "\n",
    "# 定义缩放到 0-5 区间的函数\n",
    "def scale_to_range(value, min_value, max_value, new_min, new_max):\n",
    "    # 将 value 从 (min_value, max_value) 区间缩放到 (new_min, new_max) 区间\n",
    "    return ((value - min_value) / (max_value - min_value)) * (new_max - new_min) + new_min\n",
    "\n",
    "# 读取CSV文件并逐行处理\n",
    "with open(csv_file_path, 'r', encoding='utf-8') as csv_file, open(output_csv_file_path, 'w', newline='', encoding='utf-8') as output_csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    fieldnames = csv_reader.fieldnames + ['sentiment_comment_point_sum']  # 在表头中增加 sentiment_comment_point_sum 列\n",
    "    csv_writer = csv.DictWriter(output_csv_file, fieldnames=fieldnames)\n",
    "    \n",
    "    # 写入表头\n",
    "    csv_writer.writeheader()\n",
    "    \n",
    "    # 处理每一行数据\n",
    "    for row in csv_reader:\n",
    "        comment_point = float(row['comment_point'])  # 按名字获取 comment_point\n",
    "        sentiment_score = row['sentiment_score']  # 按名字获取 sentiment_score\n",
    "\n",
    "        if sentiment_score == 'null':  # 如果 sentiment_score 是 'null'\n",
    "            sentiment_comment_point_sum = comment_point\n",
    "        else:\n",
    "            sentiment_score = float(sentiment_score)\n",
    "            # 计算 sentiment_score 和 comment_point 的和\n",
    "            total = sentiment_score + comment_point\n",
    "            \n",
    "            # 假设 sentiment_score 在 [-1, 1]，comment_point 在 [0, 5]\n",
    "            # total 的可能范围是 [-1, 6]，我们将其缩放到 [0, 5] 区间\n",
    "            sentiment_comment_point_sum = scale_to_range(total, -1, 6, 0, 5)\n",
    "\n",
    "        # 保留两位小数\n",
    "        sentiment_comment_point_sum = round(sentiment_comment_point_sum, 2)\n",
    "\n",
    "        # 在行中添加 sentiment_comment_point_sum 列\n",
    "        row['sentiment_comment_point_sum'] = sentiment_comment_point_sum\n",
    "        csv_writer.writerow(row)\n",
    "\n",
    "print(f\"处理完成，结果已保存到 {output_csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有行的 sentiment_comment_point_sum 列都有值。\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# 文件路径\n",
    "csv_file_path = './datasets/final_mapogu-4k-ce7-fd6_row_id_scaled.csv'  # 已经生成的CSV文件路径\n",
    "\n",
    "# 计数器来跟踪是否有空值\n",
    "empty_rows = []\n",
    "\n",
    "# 读取CSV文件并逐行处理\n",
    "with open(csv_file_path, 'r', encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    \n",
    "    # 逐行检查 sentiment_comment_point_sum 列是否为空\n",
    "    for row_num, row in enumerate(csv_reader, start=1):\n",
    "        sentiment_comment_point_sum = row.get('sentiment_comment_point_sum', '').strip()\n",
    "        \n",
    "        # 检查是否为空值或为 'null'\n",
    "        if sentiment_comment_point_sum == '' or sentiment_comment_point_sum == 'null':\n",
    "            empty_rows.append(row_num)  # 记录空缺行的行号\n",
    "\n",
    "# 输出结果\n",
    "if empty_rows:\n",
    "    print(f\"发现 sentiment_comment_point_sum 列存在空缺值的行号: {empty_rows}\")\n",
    "else:\n",
    "    print(\"所有行的 sentiment_comment_point_sum 列都有值。\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_tracker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
